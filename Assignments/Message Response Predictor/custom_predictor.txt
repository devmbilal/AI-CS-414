import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from datetime import datetime
from dateutil import parser
from textblob import TextBlob 
from sklearn.preprocessing import OneHotEncoder
from dateutil import parser
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


epsilon = 1e-15


# Function to initialize parameters
def initialize_parameters(dim):
    w = np.zeros((dim, 1))
    b = 0
    return w, b

# Function to compute the sigmoid activation
def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

# Function to compute the cost (logistic loss)
def compute_cost(X, y, w, b):
    m = X.shape[0]
    A = sigmoid(np.dot(X, w) + b)
    cost = -1/m * np.sum(y * np.log(A + epsilon) + (1 - y) * np.log(1 - A + epsilon))
    return cost

# Function to perform gradient descent and update parameters
def gradient_descent(X, y, w, b, learning_rate, num_iterations):
    m = X.shape[0]
    costs = []

    for i in range(num_iterations):
        # Forward pass
        A = sigmoid(np.dot(X, w) + b)

        # Compute gradients
        dw = 1/m * np.dot(X.T, (A - y))
        db = 1/m * np.sum(A - y)

        # Update parameters
        w -= learning_rate * dw
        b -= learning_rate * db

        # Compute and record the cost
        cost = compute_cost(X, y, w, b)
        costs.append(cost)

    return w, b, costs

# Function to make predictions
def predict(X, w, b):
    A = sigmoid(np.dot(X, w) + b)
    predictions = (A > 0.5).astype(int)
    return predictions


def extract_features(messages, dates, times, senders):
    features = []

    # Initialize current_datetime with the first valid date and time
    current_datetime = parser.parse(f"{dates[0]} {times[0]}")

    # Initialize one-hot encoders
    time_of_day_encoder = OneHotEncoder(sparse=False, categories='auto')
    sender_encoder = OneHotEncoder(sparse=False, categories='auto')

    # Fit the one-hot encoders on the entire dataset
    time_of_day_encoded = time_of_day_encoder.fit_transform([[time] for time in ["morning", "afternoon", "evening"]])
    sender_encoded = sender_encoder.fit_transform(np.array(senders).reshape(-1, 1))

    for i in range(len(messages)):
        # Feature 1: Message Length
        length_feature = len(messages[i])

        # # Feature 2: Time Gap (assuming date and time are in appropriate formats)
        # if i > 0:
        #     current_datetime = parser.parse(f"{dates[i]} {times[i]}")
        #     previous_datetime = parser.parse(f"{dates[i-1]} {times[i-1]}")
        #     time_gap_feature = (current_datetime - previous_datetime).total_seconds()
        # else:
        #     time_gap_feature = 0

        # Feature 3: Keywords
        keywords = ["yes", "no", "sure"]  # Add more keywords as needed
        keyword_feature = any(keyword in messages[i].lower() for keyword in keywords)

        # Feature 4: Sentiment Analysis
        blob = TextBlob(messages[i])
        sentiment_feature = blob.sentiment.polarity  # Range between -1 and 1

        # Feature 5: Content Feature (presence of specific characters or symbols)
        content_characters = ["@", "#", "!"]  # Add more characters as needed
        content_feature = any(char in messages[i] for char in content_characters)

        # Feature 6: Time of Day Feature
        hour_of_day = current_datetime.hour
        if 6 <= hour_of_day < 12:
            time_of_day_feature = "morning"
        elif 12 <= hour_of_day < 18:
            time_of_day_feature = "afternoon"
        else:
            time_of_day_feature = "evening"

        # Feature 7: Sender Name (Categorical Encoding)
        sender_feature = senders[i]

        # Apply one-hot encoding to categorical features
        time_of_day_encoded_sample = time_of_day_encoded[["morning", "afternoon", "evening"].index(time_of_day_feature)].tolist()
        sender_encoded_sample = sender_encoded[i].tolist()

        # Add features to the feature vector
        features.append([
            length_feature, keyword_feature, sentiment_feature,
            content_feature, *time_of_day_encoded_sample, *sender_encoded_sample
        ])

    return np.array(features, dtype=np.float64)




# Assuming your CSV has columns named 'Date', 'Time', 'Sender', 'Message'
# Extract features and preprocess the data
data_path = "/content/dataset.csv"
df = pd.read_csv(data_path)

# Display the column names in your dataset
print("Column Names:", df.columns)

# Convert 'Sender' column to numeric using label encoding
label_encoder = LabelEncoder()
df['Sender'] = label_encoder.fit_transform(df['Sender'])

# Adjust column names
X = extract_features(df['Message'], df['Date'], df['Time'], df['Sender'])
y = df['Sender'].values.reshape(-1, 1)

# Initialize parameters
w, b = initialize_parameters(X.shape[1])

# Set hyperparameters
learning_rate = 0.01
num_iterations = 1000

# Perform gradient descent
w, b, costs = gradient_descent(X, y, w, b, learning_rate, num_iterations)

# Plot the learning curve
plt.plot(range(num_iterations), costs)
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Learning Curve')
plt.show()

# Make predictions on the training set
predictions_train = predict(X, w, b)

# Evaluate the model on the training set
accuracy_train = np.mean(predictions_train == y)
print("Training Accuracy:", accuracy_train)



# Step 5 and 6


# Plot Learning Curve
def plot_learning_curve(iterations, costs):
    plt.plot(iterations, costs)
    plt.title('Learning Curve')
    plt.xlabel('Number of Iterations')
    plt.ylabel('Loss')
    plt.show()

# Assuming 'iterations' and 'costs' are obtained from your training process
plot_learning_curve(range(1, num_iterations + 1), costs)

# Now, experiment with different step sizes and plot their learning curves
# Define a list of different step sizes
step_sizes = [0.01, 0.1, 0.5, 1.0]

# Initialize a subplot for multiple plots
plt.figure(figsize=(12, 8))

for step_size in step_sizes:
    # Re-initialize parameters
    w, b = initialize_parameters(X.shape[1])

    # Perform gradient descent with the current step size
    w, b, costs = gradient_descent(X, y, w, b, step_size, num_iterations)

    # Assuming 'iterations' and 'costs' are obtained from training with the current step size
    plt.plot(range(1, num_iterations + 1), costs, label=f'Step Size: {step_size}')

plt.title('Learning Curves for Different Step Sizes')
plt.xlabel('Number of Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()






